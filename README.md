# ML Optimization for Ad Bidding - Project Structure

## Overview

This project implements a machine learning pipeline for optimizing keyword bids in Google Ads. It includes data preprocessing, model training, and bid optimization using linear programming.

## Directory Structure

```
project/
├── data/                           # All data files (organized by type)
│   ├── reports/                    # Keyword reports
│   │   ├── Search keyword report (by Day 2024).xlsx
│   │   └── Search keyword report (By day 2025).csv
│   ├── clean/                      # Processed data (generated by pipeline)
│   │   ├── ad_opt_data_tfidf.csv      # Full dataset with TF-IDF embeddings
│   │   ├── ad_opt_data_bert.csv       # Full dataset with BERT embeddings
│   │   ├── train_tfidf.csv            # Training set (TF-IDF)
│   │   ├── test_tfidf.csv             # Test set (TF-IDF)
│   │   ├── train_bert.csv             # Training set (BERT)
│   │   └── test_bert.csv              # Test set (BERT)
│   ├── embeddings/                 # Unique keyword embeddings
│   │   ├── unique_keyword_embeddings_tfidf.csv
│   │   └── unique_keyword_embeddings_bert.csv
│   └── combined_kw_ads_data2.csv  # Google Keyword Planner data
│
├── models/                         # Trained models (generated)
│   ├── lr_conversion.json         # Linear regression (conversion)
│   ├── lr_clicks.json             # Linear regression (clicks)
│   ├── oct_conversion.json        # Optimal trees (conversion)
│   ├── oct_clicks.json            # Optimal trees (clicks)
│   ├── rf_conversion.json         # Random forest (conversion)
│   ├── rf_clicks.json             # Random forest (clicks)
│   ├── xgb_conversion.json        # XGBoost (conversion)
│   └── xgb_clicks.json            # XGBoost (clicks)
│
├── notebooks/                      # Jupyter notebooks (reference/development)
│   ├── tidy_get_data.ipynb        # Data exploration & preprocessing
│   ├── tidy_get_data_tfidf.ipynb  # TF-IDF embedding generation
│   ├── prediction_modeling.ipynb  # Model training (Julia)
│   └── bid_optimization.ipynb     # Bid optimization (Julia)
│
├── scripts/                        # Python production scripts
│   ├── tidy_get_data.py           # Data preparation pipeline (main entry)
│   ├── prediction_modeling.py     # Model training (requires IAI library)
│   ├── bid_optimization.py        # Bid optimization (requires Gurobi)
│   └── README.md                  # Scripts documentation
│
├── utils/                          # Reusable utility modules
│   ├── data_cleaning.py           # Currency & percentage parsing
│   ├── date_features.py           # Temporal feature extraction
│   ├── embeddings.py              # TF-IDF & BERT embeddings
│   ├── data_pipeline.py           # High-level pipeline functions
│   ├── __init__.py                # Central export point
│   └── README.md                  # Utils documentation
│
├── helpers.py                      # Backward-compatible re-exports (deprecated)
├── pyproject.toml                  # Project configuration & dependencies
├── README.md                       # This file
└── .git/                           # Git repository
```

## Quick Start

### 1. Environment Setup

#### Option A: Using Conda (Recommended)

```bash
# Create conda environment with Python 3.10+
conda create -n mlopt python=3.10

# Activate environment
conda activate mlopt

# Clone repository
git clone https://github.com/josephine-situ/ad_opt.git
cd ad_opt

# Install all dependencies (core + optional)
pip install -e ".[bert,ml_open,optimization]"
```

#### Option B: Using venv

```bash
# Clone repository
git clone https://github.com/josephine-situ/ad_opt.git
cd ad_opt

# Create virtual environment
python -m venv venv

# Activate environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install all dependencies
pip install -e ".[bert,ml_open,optimization]"
```

**Dependency Installation Details:**

```bash
# Core only (data processing, embeddings)
pip install -e .

# Optional packages:
pip install -e ".[bert]"           # BERT embeddings (sentence-transformers)
pip install -e ".[ml]"             # IAI model training (requires license)
pip install -e ".[ml_open]"        # Open-source model training (no license)
pip install -e ".[optimization]"   # Gurobi optimization (requires license)

# All optional packages (recommended for full pipeline)
pip install -e ".[bert,ml_open,optimization]"

# Development tools
pip install -e ".[dev]"            # pytest, black, pylint, mypy
```

**Note on Commercial Licenses:**
- **IAI (InterpretableAI):** Optional (only needed for the legacy IAI-based training workflow). Get license from https://www.interpretable.ai/
- **Gurobi:** Required for bid optimization. Get license from https://www.gurobi.com/

The script will automatically detect if running on the Engaging cluster (SLURM) and configure IAI appropriately.

### 2. Running the Full Pipeline

Once setup is complete, run the pipeline end-to-end:

```bash
# 1. Prepare data with TF-IDF embeddings
python scripts/tidy_get_data.py --embedding-method tfidf

# 2. Train prediction models (conversion value)
python scripts/prediction_modeling_tweedie.py --target conversion --embedding-method tfidf

# 3. Train prediction models (clicks)
python scripts/prediction_modeling_tweedie.py --target clicks --embedding-method tfidf

# 4. Optimize bids
python scripts/bid_optimization.py \
  --embedding-method tfidf \
  --budget 68096.51 \
  --max-bid 100

# Results will be saved to opt_results/ directory
```

**Alternative: Using BERT embeddings (slower but more accurate)**

```bash
# Replace --embedding-method tfidf with --embedding-method bert
python scripts/tidy_get_data.py --embedding-method bert
python scripts/prediction_modeling_tweedie.py --target conversion --embedding-method bert
python scripts/prediction_modeling_tweedie.py --target clicks --embedding-method bert
python scripts/bid_optimization.py --embedding-method bert --budget 68096.51
```

### 3. Data Preparation Details

The `tidy_get_data.py` script handles the full data pipeline:

```bash
python scripts/tidy_get_data.py --embedding-method tfidf --force-reload
```

This script:
- Loads and combines 2024 and 2025 keyword data from `data/reports/`
- Cleans currency, percentages, and text
- Extracts temporal features (holidays, weekends, days to course start)
- Retrieves monthly search volume from Google Keyword Planner data
- Calculates time series statistics (3-month/6-month averages, momentum)
- Generates keyword embeddings (TF-IDF or BERT)
- Imputes missing values
- Removes rows with remaining missing values
- Creates train/test splits (75/25)
- Saves processed datasets and embedding models to `data/clean/` and `models/`

**Output files:**
```
data/clean/
├── ad_opt_data_tfidf.csv           # Full dataset (~71k rows × 60 cols)
├── train_tfidf.csv                 # Training set (~53k rows)
├── test_tfidf.csv                  # Test set (~18k rows)
├── unique_keyword_embeddings_tfidf.csv

models/
├── tfidf_pipeline_50d.pkl          # Saved TF-IDF vectorizer, SVD, normalizer
└── (embedding pipeline for inference)
```

**Options:**
```bash
# Force reload all caches (useful if data changes)
python scripts/tidy_get_data.py --embedding-method tfidf --force-reload

# Use BERT embeddings (slower)
python scripts/tidy_get_data.py --embedding-method bert
```

### 4. Train Prediction Models

Train models to predict conversion value or clicks:

```bash
# Predict conversion value (with Clicks as a predictor)
python scripts/prediction_modeling_tweedie.py --target conversion

# Predict clicks
python scripts/prediction_modeling_tweedie.py --target clicks

# Train only specific models
python scripts/prediction_modeling_tweedie.py --target conversion --models glm

# Use BERT embeddings
python scripts/prediction_modeling_tweedie.py --target conversion --embedding-method bert

To also train an open-source XGBoost Tweedie model for comparison:

```bash
pip install -e ".[ml_open]"
python scripts/prediction_modeling_tweedie.py --target conversion --embedding-method tfidf --models glm xgb
```

**Legacy (IAI-based) training script**

If you have an IAI license and want to reproduce OCT / IAI RF / IAI XGB results:

```bash
pip install -e ".[ml]"
python scripts/prediction_modeling.py --target conversion --embedding-method tfidf
```
```

Trains and compares:
- Linear Regression (LR) with feature selection
- Optimal Regression Trees (ORT)
- Random Forests (RF)
- XGBoost (XGB)

**Requirements:** IAI library (included in `pip install -e ".[ml]"`)

**Output:** Models saved to `models/` directory:
```
models/
├── lr_tfidf_conversion.json
├── ort_tfidf_conversion.json
├── rf_tfidf_conversion.json
├── xgb_tfidf_conversion.json
├── lr_tfidf_clicks.json
├── ort_tfidf_clicks.json
├── rf_tfidf_clicks.json
├── xgb_tfidf_clicks.json
└── (weight CSVs for each model)
```

### 5. Optimize Bids

Use trained models to optimize keyword bids:

```bash
python scripts/bid_optimization.py \
  --embedding-method tfidf \
  --alg-conv lr \
  --alg-clicks ort \
  --budget 68096.51 \
  --max-bid 100 \
  --models-dir models
```

Uses Gurobi linear programming to maximize profit:
- Loads new keywords from `data/gkp/keywords_classified.csv`
- Generates embeddings using saved pipeline
- Loads trained models for conversion and clicks prediction
- Solves optimization problem subject to budget and bid constraints

**Requirements:** Gurobi solver (included in `pip install -e ".[optimization]"`)

**Output:** Optimized bids saved to `opt_results/` directory

## Modules

### `utils/data_cleaning.py`
Utilities for parsing and normalizing data:
- `clean_currency()` - Parse currency strings
- `convert_percent_to_float()` - Parse percentage values

### `utils/date_features.py`
Temporal feature extraction:
- `_is_holiday()` - Detect public holidays
- `calculate_days_to_next()` - Days until next event
- `_region_to_country_code()` - Map regions to country codes

### `utils/embeddings.py`
Keyword embedding generation:
- `get_tfidf_embeddings()` - TF-IDF with TruncatedSVD (fast, interpretable)
- `get_bert_embeddings_pipeline()` - BERT with TruncatedSVD (semantic, accurate)
- `get_bert_embedding()` - Raw BERT embeddings (low-level)

### `utils/data_pipeline.py`
High-level pipeline functions (called by `scripts/tidy_get_data.py`):
- `load_and_combine_keyword_data()`
- `format_keyword_data()`
- `extract_date_features()`
- `merge_with_ads_data()`
- `add_embeddings()`
- `prepare_train_test_split()`
- `save_outputs()`

## Features

### Input Data
- Google Ads performance data (2024-2025)
- Google Keyword Planner data (search volume, competition, bid ranges)

### Engineered Features
**Temporal:**
- Day of week
- Weekend indicator
- Month
- Public holiday indicator
- Days to next course start

**Text/Semantic:**
- TF-IDF embeddings (50 dimensions)
- Or BERT embeddings (50 dimensions after SVD)

**Bid Features:**
- Average CPC
- Average bid (low + high range / 2)

**Categorical:**
- Match type (broad, exact, phrase)
- Region (USA, Region A, B, C)

### Target Variables
- **Conversion Value:** Revenue generated by conversion
- **Clicks:** Number of ad clicks

## Model Performance

Example results (MSE on test set):
```
Conversion Value Prediction:
  Linear Regression:  ~2500
  Optimal Trees:      ~2200
  Random Forest:      ~1800
  XGBoost:            ~1600

Click Prediction:
  Linear Regression:  ~15
  Optimal Trees:      ~12
  Random Forest:      ~8
  XGBoost:            ~6
```

## Optimization

The bid optimization formulates the problem as:

```
maximize:   sum(predicted_conversion - predicted_clicks * bid)
subject to:
  sum(bid) <= budget_limit
  bid[i] <= max_bid * active[i]
  sum(active) <= max_active_keywords
  bid >= 0, active ∈ {0, 1}
```

Where:
- `bid[i]` = CPC bid for keyword i
- `active[i]` = binary variable indicating if keyword is active
- Predictions come from trained models

## Dependencies

Dependencies are managed via `pyproject.toml`. Install using pip with extras:

```bash
# Core dependencies only (data processing, embeddings)
pip install -e .

# Add optional packages as needed
pip install -e ".[bert]"           # BERT embeddings
pip install -e ".[ml]"             # IAI model training
pip install -e ".[optimization]"   # Gurobi optimization
pip install -e ".[dev]"            # Development tools

# All optional packages
pip install -e ".[bert,ml,optimization,dev]"
```

**Core Packages:**
```
pandas>=1.5.3          # Data manipulation
numpy>=1.24.3          # Numerical computing
scikit-learn>=1.2.2    # ML utilities (TF-IDF, train/test split)
scipy>=1.10.1          # Scientific computing
openpyxl>=3.0.10       # Excel file reading
holidays>=0.83         # Holiday calendar detection
torch>=2.0.0           # Deep learning backend
```

**Optional Packages:**
```
# BERT embeddings
sentence-transformers>=5.1.2       # Pre-trained BERT models
transformers>=4.35.2               # HuggingFace transformers (Python 3.9 compatible)

# Model training
iai>=2.11.2                        # InterpretableAI (requires commercial license)

# Bid optimization
gurobipy>=10.0.1                   # Gurobi solver (requires commercial license)

# Development
pytest>=7.3.1                      # Testing
black>=23.3.0                      # Code formatting
pylint>=2.12.2                     # Linting
mypy>=1.0.0                        # Type checking
```

**Note on Commercial Licenses:**
- **IAI:** https://www.interpretable.ai/
- **Gurobi:** https://www.gurobi.com/

## Cluster Support

The `prediction_modeling.py` script automatically detects and configures for the **Engaging cluster** (MIT's SLURM-based cluster):

```bash
# On local machine
python scripts/prediction_modeling.py --target conversion
# Uses: C:\Users\jsitu\IAI\sys.dll (Windows)

# On Engaging cluster (automatically detected via SLURM_NODEID)
python scripts/prediction_modeling.py --target conversion
# Uses: ~/iai/sys.so and Julia from /orcd/software/community/001/pkg/julia/1.10.4/bin/julia
```

No configuration changes needed - the script handles both environments automatically!

## Performance Notes

- **Data preparation:** ~30-60 seconds (TF-IDF), ~2-3 minutes (BERT with GPU)
- **Model training:** ~1-2 minutes per model (5-fold CV)
- **Bid optimization:** ~30 seconds to 2 minutes (depending on Gurobi settings)

## Troubleshooting

### Issue: `ModuleNotFoundError: No module named 'utils'`
**Solution:** Ensure you're running scripts from the project root directory:
```bash
cd /path/to/mlopt-final
python scripts/tidy_get_data.py
```

### Issue: `ImportError: No module named 'iai'`
**Solution:** Install IAI:
```bash
pip install iai
# Note: Requires a valid InterpretableAI license
```

### Issue: `ImportError: No module named 'gurobipy'`
**Solution:** Install Gurobi:
```bash
pip install gurobipy
# Note: Requires a valid Gurobi license
```

### Issue: IAI/Gurobi license errors
**Solution:** Obtain and configure licenses:
- IAI: https://www.interpretable.ai/
- Gurobi: https://www.gurobi.com/

## Workflow

Typical workflow:

```bash
# 1. Prepare data
python scripts/tidy_get_data.py --embedding-method tfidf

# 2. Train models (requires IAI)
python scripts/prediction_modeling.py --target conversion
python scripts/prediction_modeling.py --target clicks

# 3. Optimize bids (requires Gurobi)
python scripts/bid_optimization.py --budget 68096.51

# 4. Review results
head -20 optimized_bids.csv
```

## Further Development

### Potential Improvements
1. **Hyperparameter tuning** - Expand grid search ranges
2. **Ensemble methods** - Combine multiple models
3. **Stochastic optimization** - Account for uncertainty in predictions
4. **Multi-period optimization** - Dynamic bidding over time
5. **A/B testing framework** - Validate recommendations

### Adding New Features
1. Add feature extraction functions to `utils/date_features.py`
2. Update `utils/data_pipeline.py` to compute new features
3. Retrain models with new feature matrix

## References

- Google Ads API: https://developers.google.com/google-ads/api
- InterpretableAI (IAI): https://www.interpretable.ai/
- Gurobi Optimizer: https://www.gurobi.com/
- Scikit-learn: https://scikit-learn.org/
- Sentence-Transformers: https://www.sbert.net/

## License

[Your license here]

## Contact

[Your contact information]
